---
title: "Twitter Analysis of AlphaGo"
author: "Liuqian Zhang"
date: "Dec 4th, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) 
pacman::p_load(
  "devtools",
  "twitteR",
  "ggplot2",
  "tidyverse",
  "tidytext",
  "wordcloud",
  "reshape2",
  "lubridate"

)


```

## Connect to twitter api
```{r}
api_key <- 	"Y96v0OOcyQfNb6Xtdq5OMKh65"
api_secret <- "QMamXhqcqyfGMDMtHhZVECAdvEvDJ0t2Tq406382qaUVA7YWgl"
access_token <- "927637446975770624-0HMkuqqvY4uONQH0Hk61UGjDvjSoyRV"
access_token_secret <- "VdyedkmyhuCLV1aOM9FvxRBT4EO1bAcb8BRYBA7tJdElA"
  


twitteR::setup_twitter_oauth(api_key, 
                    api_secret, 
                    access_token, 
                    access_token_secret)

```
## read data
```{r}
data <- searchTwitter('alphago',since='2017-01-01', until='2017-12-05',lang = "en", n=1000) %>% twListToDF()
head(data)

# write.csv(data, "AlphaGo.csv")
```
## Text Analysis of Top Words
### Word Frequency
```{r}
## tokenize

tidy_data <- data %>% unnest_tokens(word,text)

## remove stop words

data(stop_words)

tidy_data <-  tidy_data %>% 
  anti_join(stop_words)

tidy_data %>%  count(word, sort=TRUE)


tidy_data %>%
  count(word, sort = TRUE) %>%
  filter(n > 30) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```
### Sentiment Analysis
```{r}
get_sentiments("afinn")

get_sentiments("bing")

get_sentiments("nrc")
```

```{r}
#positive and negtive words

nrcpos <- get_sentiments("nrc") %>% 
  filter(sentiment == "positive")

tidy_data %>%
  inner_join(nrcpos) %>%
  count(word, sort = TRUE)


nrcneg <- get_sentiments("nrc") %>% 
  filter(sentiment == "negative")

tidy_data %>%
  inner_join(nrcneg) %>%
  count(word, sort = TRUE)

nrcfear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_data %>%
  inner_join(nrcfear) %>%
  count(word, sort = TRUE)

nrcjoy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_data %>%
  inner_join(nrcjoy) %>%
  count(word, sort = TRUE)
```
The word "mother" appear in both lists, so we need to fix that by adding it to stopword list. The word "learning" appear in positive words, but in this context, it mostly related to machine learning, which is a neutural word. So we add it to positive stop words. On the other hand, the word "player" appears in negative lists. I don't consider this as negative word in this case, because it can be used to describe players of the Go game. Need to add it to the negative stop words list.

```{r}
custom_stop_words <- bind_rows(data_frame(word = c("mother", "player","learning","intelligence"), 
                                          lexicon = c("custom")), 
                               stop_words)

(fearlist <- tidy_data %>%
  anti_join(custom_stop_words) %>%
  inner_join(nrcfear) %>%
  count(word, sort = TRUE))

(joylist <- tidy_data %>%
  anti_join(custom_stop_words) %>%
  inner_join(nrcjoy) %>%
  count(word, sort = TRUE))

fearlist %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill="blue") +
  xlab(NULL) +
  coord_flip() +
  scale_y_continuous(limits = c(0,110))

joylist %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill="red") +
  xlab(NULL) +
  coord_flip() +
  scale_y_continuous(limits = c(0,110))
```
From the twitters, the word "excited" are mentioned over 100 time, indicating it is an important sentiment of people towards AlphaGo.

The word "bad" appeared over 40 times. This is also an important sentiment here. Some words like "threat", and "nervous" may indicate something about people's worry. But because the sample size of the twitters is not large enough, these words only appear once. We cannot say whether people feel fear about AlphaGo.
## sentiment analysis with bing
```{r}
bingpos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")

(poslist <- tidy_data %>%
  anti_join(custom_stop_words) %>%
  inner_join(bingpos) %>%
  count(word, sort = TRUE))


bingneg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

(neglist <- (tidy_data %>%
  inner_join(bingneg) %>%
  anti_join(custom_stop_words) %>%
  count(word, sort = TRUE)))

poslist %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill="red") +
  xlab(NULL) +
  coord_flip() +
  scale_y_continuous(limits = c(0,110))

neglist %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill="blue") +
  xlab(NULL) +
  coord_flip() +
  scale_y_continuous(limits = c(0,110))
```


```{r}
library(wordcloud)


tidy_data %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

```{r}
  tidy_data %>%
  anti_join(custom_stop_words) %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)
```




From the first word cloud, we can see during this period, people talk a lot about the alphago documentary. And the url of the movie appears many times.

## How hot is the topic during the period?
```{r}
# by date
(dates <- data %>%
  mutate(date=date(created)) %>%
  group_by(date) %>%
  summarise(date_n = n()))

ggplot() +
  geom_bar(data=dates, 
           aes(date,date_n),
           stat = "identity", 
           fill="#1DA1F2") +
  ylab("Number of tweets") +
  scale_x_discrete(limits=lubridate::date(dates$date),labels=lubridate::date(dates$date))
```
The number of tweets about AlphaGo in December 2nd and 3rd are higher than other dates.

```{r}
# First 10 rows The tweets in dec 2nd

data %>%
  filter(date(created)==c("2017-12-02")) %>%
  .$text %>%
  head(10)

data %>%
  filter(date(created)==c("2017-12-02")) %>%
  nrow()

```
```{r}
# number of tweets containing GooglePlay in dec 2nd
data %>%
  filter(date(created)==c("2017-12-02")) %>%
  filter(grepl("GooglePlay",text)) %>%
  nrow

paste(round(87/144*100,2), "% of the tweets in Dec 2nd are talking about AlphaGo movie available on GooglePlay.")
```
```{r}
# number of tweets containing documentary in dec 2nd
data %>%
  filter(date(created)==c("2017-12-02")) %>%
  filter(grepl("documentary",text)) %>%
  nrow

```


```{r}
# by hour
(hourly <- data %>%
  mutate(hour=hour(created)) %>%
  group_by(hour) %>%
  summarise(hour_n = n()))

label1 <-seq(0,23,1)

ggplot() +
  geom_line(data=hourly, 
           aes(hour,hour_n/7),
           stat = "identity", 
           color="#1DA1F2",
           size=1) +
  ylab("Average number of tweets") +
  scale_x_discrete(limits=label1)


```
The average number of tweets about AlphaGo is peaked at 8am, 11am, and 2pm.
